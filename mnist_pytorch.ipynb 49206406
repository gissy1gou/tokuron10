{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_pytorch.ipynb 49206406","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FEFfggiRdvrw","colab_type":"text"},"source":["##MNIST pytorch版"]},{"cell_type":"markdown","metadata":{"id":"3gD8e4KPknr1","colab_type":"text"},"source":["pytorch例題を使用。"]},{"cell_type":"code","metadata":{"id":"rRQ2Nqoxp89O","colab_type":"code","colab":{}},"source":["!git clone https://github.com/pytorch/examples.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLS1XMXwky7_","colab_type":"text"},"source":["mnistプログラムを探す。"]},{"cell_type":"code","metadata":{"id":"MN69FZfqqRqi","colab_type":"code","colab":{}},"source":["%cd examples/\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3Xm6nvAqV5M","colab_type":"code","colab":{}},"source":["%cd mnist/\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ClxRYpXtk6ES","colab_type":"text"},"source":["READMEを読む。"]},{"cell_type":"code","metadata":{"id":"K3kaAp94qbH1","colab_type":"code","colab":{}},"source":["!cat README.md"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYt_m9z7lCd7","colab_type":"text"},"source":["tourchとtorchvisionをインストール"]},{"cell_type":"code","metadata":{"id":"WcTcq7IyfTHu","colab_type":"code","colab":{}},"source":["!cat requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7R_jpX-qfqN","colab_type":"code","colab":{}},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdSd-3KpeSJY","colab_type":"text"},"source":["main.py"]},{"cell_type":"code","metadata":{"id":"x-qqgogmPXCZ","colab_type":"code","colab":{}},"source":["!pip install tqdm\n","import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L0JoBJcPZzgF","colab":{}},"source":["#Python2系との互換。無視\n","#from __future__ import print_function\n","\n","#コマンドラインを解析し、Pythonデータ型に変換\n","import argparse\n","\n","#torchライブラリーを呼び込む\n","import torch\n","\n","#WHAT IS TORCH.NN REALLY?\n","#ref. https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","import torch.nn as nn\n","\n","#refactoring bare codes\n","import torch.nn.functional as F\n","\n","#最適化パッケージ\n","import torch.optim as optim\n","\n","# Pytorchが提供する画像データおよびPytorch用データ変換ライブラリ\n","from torchvision import datasets, transforms\n","\n","#エポック単位での学習率変化を自動計算\n","#1epock=(データ60000/(バッチサイズ64*log-interval(=10))単位で損失計算\n","# ⇒全単位終了後の損失平均\n","from torch.optim.lr_scheduler import StepLR\n","\n","#pytorchネットワークモジュール定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #入力チャネル=1(白黒仮定), 出力チャネル=32, カーネルサイズ=3, ストライド=1\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        #forward propagationでは確率p(=0.25)で入力が０となり、\n","        #それ以外は入力が1/(1-p)でスケールアップする。\n","        #back propagationでは勾配に対し確率pで０化され、それ以外は1/(1-p)でスケールアップする。\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        #入力特徴数=9216、出力特徴数=128 全結合ネットワーク\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10) # 線形にする\n","\n","    def forward(self, x):\n","        #２回にわたり特徴面を拡大。\n","        #InputShape=28*28*1 OutputShape=26*26*32, Weights=(3*3*1)*32=288\n","        x = self.conv1(x)\n","        #torchが提供するランプ関数 負部分の値をゼロに平滑化する\n","        x = F.relu(x)\n","        #InputShape=26*26*32, OutputShape=24*24*64, Weights=(3*3*32)*64=18342\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        #カーネルサイズ=2 (入力2*2範囲内の最大値を1セルで置き換える)\n","        #InputShape=24*24*64, OutputShape=12*12*64(=9216)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        #InputShape=12*12*64, OutputShape=9216\n","        x = torch.flatten(x, 1)\n","        #Weight=9216*128=1179648\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        #Weghts=128*10=1280\n","        x = self.fc2(x)\n","        #-- Total Weights=1199558~=1.2M parameters\n","        #解説\n","        #softmax: e(x_i)/e(x).sum()\n","        #log_softmax: log(softmax(x))\n","        #dim=1 row-wise\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    #1エポックごとに呼び出される。\n","    #modelを学習用にセットアップする。\n","    model.train()\n","    #データをbatch-size単位に切り出し、(data, target)に再代入する\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        #data, targetをdevice（たとえばGPU）に展開する。\n","        data, target = data.to(device), target.to(device)\n","        #ミニバッチ単位の勾配計算に関し、勾配値の集積を毎回リセットする。\n","        optimizer.zero_grad()\n","        #出力を得る\n","        output = model(data)\n","        #損失を計算。 a:softmax(x), nll: -log(a)\n","        loss = F.nll_loss(output, target)\n","        #loss = F.cross_entropy(output, target)\n","        #勾配値を計算し、全パラメータを更新する。\n","        loss.backward()\n","        #指定された最適化関数の最適化方向へ勾配をシフト\n","        optimizer.step()\n","        #batch-size(=64)*log-interval(=50)ごとに損失率を記録。\n","        #loss.item() 損失平均\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","def test(model, device, test_loader):\n","    #学習時と異なるネットワーク構成（たとえばdropoutを含む）の場合。\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    #model.train()で学習済みのため、テスト時、勾配計算を省く。これによって推論時速度が上がる。\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            #test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            #出力中、もっとも高い確率を持つインデックスを知る。\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            #正解数を計算する。\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","def main():\n","    # Training settings\n","    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                        help='input batch size for training (default: 64)')\n","    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                        help='input batch size for testing (default: 1000)')\n","    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n","    # parser.add_argument('--epochs', type=int, default=14, metavar='N',\n","                        help='number of epochs to train (default: 14)')\n","    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n","                        help='learning rate (default: 1.0)') # 学習率\n","    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n","                        help='Learning rate step gamma (default: 0.7)')\n","    parser.add_argument('--no-cuda', action='store_true', default=False,\n","                        help='disables CUDA training')\n","    parser.add_argument('--dry-run', action='store_true', default=False,\n","                        help='quickly check a single pass')\n","    parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                        help='random seed (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n","                        help='how many batches to wait before logging training status')\n","    parser.add_argument('--save-model', action='store_true', default=False,\n","                        help='For Saving the current Model')\n","    ##本来、pythonインラインで実行を想定したプログラムをjupyterで動かす際の引数渡しに\n","    ##関する変更\n","    #args = parser.parse_args(), replaced with the following code for run in Jupyter\n","    args = parser.parse_args(args=[])\n","    #CUDA、つまりGPU使用の可否を確認。\n","    # use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    use_cuda = False\n","\n","    #全体で使用する乱数シードの設定。\n","    torch.manual_seed(args.seed)\n","\n","    #使用するデバイスの決定。cpuもしくはgpu\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    #cuda用学習時パラメータの用意\n","    #num_workers: 並列バッチ数\n","    kwargs = {'batch_size': args.batch_size}\n","    if use_cuda:\n","        kwargs.update({'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True},\n","                     )\n","    #入力データ(ndarray)をTensor値に変更\n","    #image = (image - mean) / std, here mena=0.1307, std=0.3081\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    #学習前データ\n","    dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","    #学習済みデータ\n","    dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)\n","    #ミニバッチを前提としたデータ取り込み\n","    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n","\n","    #netモデルをデバイスに展開\n","    model = Net().to(device)\n","    #最適化モデルの選択\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in tqdm.tqdm(range(1, args.epochs + 1)):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DN4I3Aq_bdnx","colab_type":"text"},"source":["↑　GPUを使わない"]},{"cell_type":"code","metadata":{"id":"l3aUTutLqyFE","colab_type":"code","colab":{}},"source":["#Python2系との互換。無視\n","#from __future__ import print_function\n","\n","#コマンドラインを解析し、Pythonデータ型に変換\n","import argparse\n","\n","#torchライブラリーを呼び込む\n","import torch\n","\n","#WHAT IS TORCH.NN REALLY?\n","#ref. https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","import torch.nn as nn\n","\n","#refactoring bare codes\n","import torch.nn.functional as F\n","\n","#最適化パッケージ\n","import torch.optim as optim\n","\n","# Pytorchが提供する画像データおよびPytorch用データ変換ライブラリ\n","from torchvision import datasets, transforms\n","\n","#エポック単位での学習率変化を自動計算\n","#1epock=(データ60000/(バッチサイズ64*log-interval(=10))単位で損失計算\n","# ⇒全単位終了後の損失平均\n","from torch.optim.lr_scheduler import StepLR\n","\n","#pytorchネットワークモジュール定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #入力チャネル=1(白黒仮定), 出力チャネル=32, カーネルサイズ=3, ストライド=1\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        #forward propagationでは確率p(=0.25)で入力が０となり、\n","        #それ以外は入力が1/(1-p)でスケールアップする。\n","        #back propagationでは勾配に対し確率pで０化され、それ以外は1/(1-p)でスケールアップする。\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        #入力特徴数=9216、出力特徴数=128 全結合ネットワーク\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10) # 線形にする\n","\n","    def forward(self, x):\n","        #２回にわたり特徴面を拡大。\n","        #InputShape=28*28*1 OutputShape=26*26*32, Weights=(3*3*1)*32=288\n","        x = self.conv1(x)\n","        #torchが提供するランプ関数 負部分の値をゼロに平滑化する\n","        x = F.relu(x)\n","        #InputShape=26*26*32, OutputShape=24*24*64, Weights=(3*3*32)*64=18342\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        #カーネルサイズ=2 (入力2*2範囲内の最大値を1セルで置き換える)\n","        #InputShape=24*24*64, OutputShape=12*12*64(=9216)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        #InputShape=12*12*64, OutputShape=9216\n","        x = torch.flatten(x, 1)\n","        #Weight=9216*128=1179648\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        #Weghts=128*10=1280\n","        x = self.fc2(x)\n","        #-- Total Weights=1199558~=1.2M parameters\n","        #解説\n","        #softmax: e(x_i)/e(x).sum()\n","        #log_softmax: log(softmax(x))\n","        #dim=1 row-wise\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    #1エポックごとに呼び出される。\n","    #modelを学習用にセットアップする。\n","    model.train()\n","    #データをbatch-size単位に切り出し、(data, target)に再代入する\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        #data, targetをdevice（たとえばGPU）に展開する。\n","        data, target = data.to(device), target.to(device)\n","        #ミニバッチ単位の勾配計算に関し、勾配値の集積を毎回リセットする。\n","        optimizer.zero_grad()\n","        #出力を得る\n","        output = model(data)\n","        #損失を計算。 a:softmax(x), nll: -log(a)\n","        loss = F.nll_loss(output, target)\n","        #loss = F.cross_entropy(output, target)\n","        #勾配値を計算し、全パラメータを更新する。\n","        loss.backward()\n","        #指定された最適化関数の最適化方向へ勾配をシフト\n","        optimizer.step()\n","        #batch-size(=64)*log-interval(=50)ごとに損失率を記録。\n","        #loss.item() 損失平均\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","def test(model, device, test_loader):\n","    #学習時と異なるネットワーク構成（たとえばdropoutを含む）の場合。\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    #model.train()で学習済みのため、テスト時、勾配計算を省く。これによって推論時速度が上がる。\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            #test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            #出力中、もっとも高い確率を持つインデックスを知る。\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            #正解数を計算する。\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","def main():\n","    # Training settings\n","    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                        help='input batch size for training (default: 64)')\n","    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                        help='input batch size for testing (default: 1000)')\n","    # parser.add_argument('--epochs', type=int, default=2, metavar='N',\n","    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n","                        help='number of epochs to train (default: 14)')\n","    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n","                        help='learning rate (default: 1.0)') # 学習率\n","    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n","                        help='Learning rate step gamma (default: 0.7)')\n","    parser.add_argument('--no-cuda', action='store_true', default=False,\n","                        help='disables CUDA training')\n","    parser.add_argument('--dry-run', action='store_true', default=False,\n","                        help='quickly check a single pass')\n","    parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                        help='random seed (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n","                        help='how many batches to wait before logging training status')\n","    parser.add_argument('--save-model', action='store_true', default=False,\n","                        help='For Saving the current Model')\n","    ##本来、pythonインラインで実行を想定したプログラムをjupyterで動かす際の引数渡しに\n","    ##関する変更\n","    #args = parser.parse_args(), replaced with the following code for run in Jupyter\n","    args = parser.parse_args(args=[])\n","    #CUDA、つまりGPU使用の可否を確認。\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    #use_cuda = False\n","\n","    #全体で使用する乱数シードの設定。\n","    torch.manual_seed(args.seed)\n","\n","    #使用するデバイスの決定。cpuもしくはgpu\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    #cuda用学習時パラメータの用意\n","    #num_workers: 並列バッチ数\n","    kwargs = {'batch_size': args.batch_size}\n","    if use_cuda:\n","        kwargs.update({'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True},\n","                     )\n","    #入力データ(ndarray)をTensor値に変更\n","    #image = (image - mean) / std, here mena=0.1307, std=0.3081\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    #学習前データ\n","    dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","    #学習済みデータ\n","    dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)\n","    #ミニバッチを前提としたデータ取り込み\n","    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n","\n","    #netモデルをデバイスに展開\n","    model = Net().to(device)\n","    #最適化モデルの選択\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in tqdm.tqdm(range(1, args.epochs + 1)):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4pGipX9idU5D","colab_type":"text"},"source":["↑ epoch 14"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GkRZtLAAao2q","colab":{}},"source":["#Python2系との互換。無視\n","#from __future__ import print_function\n","\n","#コマンドラインを解析し、Pythonデータ型に変換\n","import argparse\n","\n","#torchライブラリーを呼び込む\n","import torch\n","\n","#WHAT IS TORCH.NN REALLY?\n","#ref. https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","import torch.nn as nn\n","\n","#refactoring bare codes\n","import torch.nn.functional as F\n","\n","#最適化パッケージ\n","import torch.optim as optim\n","\n","# Pytorchが提供する画像データおよびPytorch用データ変換ライブラリ\n","from torchvision import datasets, transforms\n","\n","#エポック単位での学習率変化を自動計算\n","#1epock=(データ60000/(バッチサイズ64*log-interval(=10))単位で損失計算\n","# ⇒全単位終了後の損失平均\n","from torch.optim.lr_scheduler import StepLR\n","\n","#pytorchネットワークモジュール定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #入力チャネル=1(白黒仮定), 出力チャネル=32, カーネルサイズ=3, ストライド=1\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        #forward propagationでは確率p(=0.25)で入力が０となり、\n","        #それ以外は入力が1/(1-p)でスケールアップする。\n","        #back propagationでは勾配に対し確率pで０化され、それ以外は1/(1-p)でスケールアップする。\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        #入力特徴数=9216、出力特徴数=128 全結合ネットワーク\n","        # self.fc1 = nn.Linear(9216, 128)\n","        self.fc1 = nn.Linear(5408, 128)\n","        self.fc2 = nn.Linear(128, 10) # 線形にする\n","\n","    def forward(self, x):\n","        #２回にわたり特徴面を拡大。\n","        #InputShape=28*28*1 OutputShape=26*26*32, Weights=(3*3*1)*32=288\n","        x = self.conv1(x)\n","        #torchが提供するランプ関数 負部分の値をゼロに平滑化する\n","        x = F.relu(x)\n","\n","\n","\n","        #InputShape=26*26*32, OutputShape=24*24*64, Weights=(3*3*32)*64=18342\n","        # x = self.conv2(x)\n","        # x = F.relu(x)\n","\n","\n","        #カーネルサイズ=2 (入力2*2範囲内の最大値を1セルで置き換える)\n","        #InputShape=24*24*64, OutputShape=12*12*64(=9216) -> InputShape=26*26*32, OutputShape=13*13*32 = 5408\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        #InputShape=12*12*64, OutputShape=9216 -> InputShape=13*13*32, OutputShape=13*13*32=5408\n","        x = torch.flatten(x, 1)\n","        #Weight=9216*128=1179648 -> Weight = 5408 * 128(?) = 692224 \n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        #Weghts=128*10=1280 -> Weights = 128 * 10 = 1280\n","        x = self.fc2(x)\n","        #-- Total Weights=1199558~=1.2M parameters\n","        #解説\n","        #softmax: e(x_i)/e(x).sum()\n","        #log_softmax: log(softmax(x))\n","        #dim=1 row-wise\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    #1エポックごとに呼び出される。\n","    #modelを学習用にセットアップする。\n","    model.train()\n","    #データをbatch-size単位に切り出し、(data, target)に再代入する\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        #data, targetをdevice（たとえばGPU）に展開する。\n","        data, target = data.to(device), target.to(device)\n","        #ミニバッチ単位の勾配計算に関し、勾配値の集積を毎回リセットする。\n","        optimizer.zero_grad()\n","        #出力を得る\n","        output = model(data)\n","        #損失を計算。 a:softmax(x), nll: -log(a)\n","        loss = F.nll_loss(output, target)\n","        #loss = F.cross_entropy(output, target)\n","        #勾配値を計算し、全パラメータを更新する。\n","        loss.backward()\n","        #指定された最適化関数の最適化方向へ勾配をシフト\n","        optimizer.step()\n","        #batch-size(=64)*log-interval(=50)ごとに損失率を記録。\n","        #loss.item() 損失平均\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","def test(model, device, test_loader):\n","    #学習時と異なるネットワーク構成（たとえばdropoutを含む）の場合。\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    #model.train()で学習済みのため、テスト時、勾配計算を省く。これによって推論時速度が上がる。\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            #test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            #出力中、もっとも高い確率を持つインデックスを知る。\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            #正解数を計算する。\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","def main():\n","    # Training settings\n","    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                        help='input batch size for training (default: 64)')\n","    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                        help='input batch size for testing (default: 1000)')\n","    # parser.add_argument('--epochs', type=int, default=2, metavar='N',\n","    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n","                        help='number of epochs to train (default: 14)')\n","    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n","                        help='learning rate (default: 1.0)') # 学習率\n","    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n","                        help='Learning rate step gamma (default: 0.7)')\n","    parser.add_argument('--no-cuda', action='store_true', default=False,\n","                        help='disables CUDA training')\n","    parser.add_argument('--dry-run', action='store_true', default=False,\n","                        help='quickly check a single pass')\n","    parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                        help='random seed (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n","                        help='how many batches to wait before logging training status')\n","    parser.add_argument('--save-model', action='store_true', default=False,\n","                        help='For Saving the current Model')\n","    ##本来、pythonインラインで実行を想定したプログラムをjupyterで動かす際の引数渡しに\n","    ##関する変更\n","    #args = parser.parse_args(), replaced with the following code for run in Jupyter\n","    args = parser.parse_args(args=[])\n","    #CUDA、つまりGPU使用の可否を確認。\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    # use_cuda = False\n","\n","    #全体で使用する乱数シードの設定。\n","    torch.manual_seed(args.seed)\n","\n","    #使用するデバイスの決定。cpuもしくはgpu\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    #cuda用学習時パラメータの用意\n","    #num_workers: 並列バッチ数\n","    kwargs = {'batch_size': args.batch_size}\n","    if use_cuda:\n","        kwargs.update({'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True},\n","                     )\n","    #入力データ(ndarray)をTensor値に変更\n","    #image = (image - mean) / std, here mena=0.1307, std=0.3081\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    #学習前データ\n","    dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","    #学習済みデータ\n","    dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)\n","    #ミニバッチを前提としたデータ取り込み\n","    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n","\n","    #netモデルをデバイスに展開\n","    model = Net().to(device)\n","    #最適化モデルの選択\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in tqdm.tqdm(range(1, args.epochs + 1)):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y28WUDUUa_KU","colab_type":"text"},"source":["↑ epoch 14 convolution なくす\n","\n","チャネルの数が合うようにしないといけない\n","linearも"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3Py5acW-bL67","colab":{}},"source":["#Python2系との互換。無視\n","#from __future__ import print_function\n","\n","#コマンドラインを解析し、Pythonデータ型に変換\n","import argparse\n","\n","#torchライブラリーを呼び込む\n","import torch\n","\n","#WHAT IS TORCH.NN REALLY?\n","#ref. https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","import torch.nn as nn\n","\n","#refactoring bare codes\n","import torch.nn.functional as F\n","\n","#最適化パッケージ\n","import torch.optim as optim\n","\n","# Pytorchが提供する画像データおよびPytorch用データ変換ライブラリ\n","from torchvision import datasets, transforms\n","\n","#エポック単位での学習率変化を自動計算\n","#1epock=(データ60000/(バッチサイズ64*log-interval(=10))単位で損失計算\n","# ⇒全単位終了後の損失平均\n","from torch.optim.lr_scheduler import StepLR\n","\n","#pytorchネットワークモジュール定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #入力チャネル=1(白黒仮定), 出力チャネル=32, カーネルサイズ=3, ストライド=1\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        #forward propagationでは確率p(=0.25)で入力が０となり、\n","        #それ以外は入力が1/(1-p)でスケールアップする。\n","        #back propagationでは勾配に対し確率pで０化され、それ以外は1/(1-p)でスケールアップする。\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        #入力特徴数=9216、出力特徴数=128 全結合ネットワーク\n","        #self.fc1 = nn.Linear(9216, 128)\n","        self.fc1 = nn.Linear(9216, 10)\n","        #self.fc2 = nn.Linear(128, 10) # 線形にする\n","\n","    def forward(self, x):\n","        #２回にわたり特徴面を拡大。\n","        #InputShape=28*28*1 OutputShape=26*26*32, Weights=(3*3*1)*32=288\n","        x = self.conv1(x)\n","        #torchが提供するランプ関数 負部分の値をゼロに平滑化する\n","        x = F.relu(x)\n","        #InputShape=26*26*32, OutputShape=24*24*64, Weights=(3*3*32)*64=18342\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        #カーネルサイズ=2 (入力2*2範囲内の最大値を1セルで置き換える)\n","        #InputShape=24*24*64, OutputShape=12*12*64(=9216)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        #InputShape=12*12*64, OutputShape=9216\n","        x = torch.flatten(x, 1)\n","        #Weight=9216*128=1179648 -> Weight = 9216*10=92160\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        # x = self.dropout2(x)\n","        #Weghts=128*10=1280\n","        # x = self.fc2(x)\n","        #-- Total Weights=1199558~=1.2M parameters\n","        #解説\n","        #softmax: e(x_i)/e(x).sum()\n","        #log_softmax: log(softmax(x))\n","        #dim=1 row-wise\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    #1エポックごとに呼び出される。\n","    #modelを学習用にセットアップする。\n","    model.train()\n","    #データをbatch-size単位に切り出し、(data, target)に再代入する\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        #data, targetをdevice（たとえばGPU）に展開する。\n","        data, target = data.to(device), target.to(device)\n","        #ミニバッチ単位の勾配計算に関し、勾配値の集積を毎回リセットする。\n","        optimizer.zero_grad()\n","        #出力を得る\n","        output = model(data)\n","        #損失を計算。 a:softmax(x), nll: -log(a)\n","        loss = F.nll_loss(output, target)\n","        #loss = F.cross_entropy(output, target)\n","        #勾配値を計算し、全パラメータを更新する。\n","        loss.backward()\n","        #指定された最適化関数の最適化方向へ勾配をシフト\n","        optimizer.step()\n","        #batch-size(=64)*log-interval(=50)ごとに損失率を記録。\n","        #loss.item() 損失平均\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","def test(model, device, test_loader):\n","    #学習時と異なるネットワーク構成（たとえばdropoutを含む）の場合。\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    #model.train()で学習済みのため、テスト時、勾配計算を省く。これによって推論時速度が上がる。\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            #test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            #出力中、もっとも高い確率を持つインデックスを知る。\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            #正解数を計算する。\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","def main():\n","    # Training settings\n","    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                        help='input batch size for training (default: 64)')\n","    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                        help='input batch size for testing (default: 1000)')\n","    # parser.add_argument('--epochs', type=int, default=2, metavar='N',\n","    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n","                        help='number of epochs to train (default: 14)')\n","    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n","                        help='learning rate (default: 1.0)') # 学習率\n","    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n","                        help='Learning rate step gamma (default: 0.7)')\n","    parser.add_argument('--no-cuda', action='store_true', default=False,\n","                        help='disables CUDA training')\n","    parser.add_argument('--dry-run', action='store_true', default=False,\n","                        help='quickly check a single pass')\n","    parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                        help='random seed (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n","                        help='how many batches to wait before logging training status')\n","    parser.add_argument('--save-model', action='store_true', default=False,\n","                        help='For Saving the current Model')\n","    ##本来、pythonインラインで実行を想定したプログラムをjupyterで動かす際の引数渡しに\n","    ##関する変更\n","    #args = parser.parse_args(), replaced with the following code for run in Jupyter\n","    args = parser.parse_args(args=[])\n","    #CUDA、つまりGPU使用の可否を確認。\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    # use_cuda = False\n","\n","    #全体で使用する乱数シードの設定。\n","    torch.manual_seed(args.seed)\n","\n","    #使用するデバイスの決定。cpuもしくはgpu\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    #cuda用学習時パラメータの用意\n","    #num_workers: 並列バッチ数\n","    kwargs = {'batch_size': args.batch_size}\n","    if use_cuda:\n","        kwargs.update({'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True},\n","                     )\n","    #入力データ(ndarray)をTensor値に変更\n","    #image = (image - mean) / std, here mena=0.1307, std=0.3081\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    #学習前データ\n","    dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","    #学習済みデータ\n","    dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)\n","    #ミニバッチを前提としたデータ取り込み\n","    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n","\n","    #netモデルをデバイスに展開\n","    model = Net().to(device)\n","    #最適化モデルの選択\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in tqdm.tqdm(range(1, args.epochs + 1)):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ju2LBrDPbMgV","colab_type":"text"},"source":["↑ epoch 14 linear"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3dog0uSraxQN","colab":{}},"source":["#Python2系との互換。無視\n","#from __future__ import print_function\n","\n","#コマンドラインを解析し、Pythonデータ型に変換\n","import argparse\n","\n","#torchライブラリーを呼び込む\n","import torch\n","\n","#WHAT IS TORCH.NN REALLY?\n","#ref. https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","import torch.nn as nn\n","\n","#refactoring bare codes\n","import torch.nn.functional as F\n","\n","#最適化パッケージ\n","import torch.optim as optim\n","\n","# Pytorchが提供する画像データおよびPytorch用データ変換ライブラリ\n","from torchvision import datasets, transforms\n","\n","#エポック単位での学習率変化を自動計算\n","#1epock=(データ60000/(バッチサイズ64*log-interval(=10))単位で損失計算\n","# ⇒全単位終了後の損失平均\n","from torch.optim.lr_scheduler import StepLR\n","\n","#pytorchネットワークモジュール定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #入力チャネル=1(白黒仮定), 出力チャネル=32, カーネルサイズ=3, ストライド=1\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        #forward propagationでは確率p(=0.25)で入力が０となり、\n","        #それ以外は入力が1/(1-p)でスケールアップする。\n","        #back propagationでは勾配に対し確率pで０化され、それ以外は1/(1-p)でスケールアップする。\n","        self.dropout1 = nn.Dropout2d(0.10)\n","        self.dropout2 = nn.Dropout2d(0.25)\n","        #入力特徴数=9216、出力特徴数=128 全結合ネットワーク\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10) # 線形にする\n","\n","    def forward(self, x):\n","        #２回にわたり特徴面を拡大。\n","        #InputShape=28*28*1 OutputShape=26*26*32, Weights=(3*3*1)*32=288\n","        x = self.conv1(x)\n","        #torchが提供するランプ関数 負部分の値をゼロに平滑化する\n","        x = F.relu(x)\n","        #InputShape=26*26*32, OutputShape=24*24*64, Weights=(3*3*32)*64=18342\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        #カーネルサイズ=2 (入力2*2範囲内の最大値を1セルで置き換える)\n","        #InputShape=24*24*64, OutputShape=12*12*64(=9216)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        #InputShape=12*12*64, OutputShape=9216\n","        x = torch.flatten(x, 1)\n","        #Weight=9216*128=1179648\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        #Weghts=128*10=1280\n","        x = self.fc2(x)\n","        #-- Total Weights=1199558~=1.2M parameters\n","        #解説\n","        #softmax: e(x_i)/e(x).sum()\n","        #log_softmax: log(softmax(x))\n","        #dim=1 row-wise\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    #1エポックごとに呼び出される。\n","    #modelを学習用にセットアップする。\n","    model.train()\n","    #データをbatch-size単位に切り出し、(data, target)に再代入する\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        #data, targetをdevice（たとえばGPU）に展開する。\n","        data, target = data.to(device), target.to(device)\n","        #ミニバッチ単位の勾配計算に関し、勾配値の集積を毎回リセットする。\n","        optimizer.zero_grad()\n","        #出力を得る\n","        output = model(data)\n","        #損失を計算。 a:softmax(x), nll: -log(a)\n","        loss = F.nll_loss(output, target)\n","        #loss = F.cross_entropy(output, target)\n","        #勾配値を計算し、全パラメータを更新する。\n","        loss.backward()\n","        #指定された最適化関数の最適化方向へ勾配をシフト\n","        optimizer.step()\n","        #batch-size(=64)*log-interval(=50)ごとに損失率を記録。\n","        #loss.item() 損失平均\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","def test(model, device, test_loader):\n","    #学習時と異なるネットワーク構成（たとえばdropoutを含む）の場合。\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    #model.train()で学習済みのため、テスト時、勾配計算を省く。これによって推論時速度が上がる。\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            #test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            #出力中、もっとも高い確率を持つインデックスを知る。\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            #正解数を計算する。\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","def main():\n","    # Training settings\n","    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                        help='input batch size for training (default: 64)')\n","    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                        help='input batch size for testing (default: 1000)')\n","    # parser.add_argument('--epochs', type=int, default=2, metavar='N',\n","    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n","                        help='number of epochs to train (default: 14)')\n","    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n","                        help='learning rate (default: 1.0)') # 学習率\n","    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n","                        help='Learning rate step gamma (default: 0.7)')\n","    parser.add_argument('--no-cuda', action='store_true', default=False,\n","                        help='disables CUDA training')\n","    parser.add_argument('--dry-run', action='store_true', default=False,\n","                        help='quickly check a single pass')\n","    parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                        help='random seed (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n","                        help='how many batches to wait before logging training status')\n","    parser.add_argument('--save-model', action='store_true', default=False,\n","                        help='For Saving the current Model')\n","    ##本来、pythonインラインで実行を想定したプログラムをjupyterで動かす際の引数渡しに\n","    ##関する変更\n","    #args = parser.parse_args(), replaced with the following code for run in Jupyter\n","    args = parser.parse_args(args=[])\n","    #CUDA、つまりGPU使用の可否を確認。\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    # use_cuda = False\n","\n","    #全体で使用する乱数シードの設定。\n","    torch.manual_seed(args.seed)\n","\n","    #使用するデバイスの決定。cpuもしくはgpu\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    #cuda用学習時パラメータの用意\n","    #num_workers: 並列バッチ数\n","    kwargs = {'batch_size': args.batch_size}\n","    if use_cuda:\n","        kwargs.update({'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True},\n","                     )\n","    #入力データ(ndarray)をTensor値に変更\n","    #image = (image - mean) / std, here mena=0.1307, std=0.3081\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    #学習前データ\n","    dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","    #学習済みデータ\n","    dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)\n","    #ミニバッチを前提としたデータ取り込み\n","    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n","\n","    #netモデルをデバイスに展開\n","    model = Net().to(device)\n","    #最適化モデルの選択\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in tqdm.tqdm(range(1, args.epochs + 1)):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fe6AZsDga1No","colab_type":"text"},"source":["↑ dropout 減らす"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pngwSrtZmHV4","colab":{}},"source":["#Python2系との互換。無視\n","#from __future__ import print_function\n","\n","#コマンドラインを解析し、Pythonデータ型に変換\n","import argparse\n","\n","#torchライブラリーを呼び込む\n","import torch\n","\n","#WHAT IS TORCH.NN REALLY?\n","#ref. https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","import torch.nn as nn\n","\n","#refactoring bare codes\n","import torch.nn.functional as F\n","\n","#最適化パッケージ\n","import torch.optim as optim\n","\n","# Pytorchが提供する画像データおよびPytorch用データ変換ライブラリ\n","from torchvision import datasets, transforms\n","\n","#エポック単位での学習率変化を自動計算\n","#1epock=(データ60000/(バッチサイズ64*log-interval(=10))単位で損失計算\n","# ⇒全単位終了後の損失平均\n","from torch.optim.lr_scheduler import StepLR\n","\n","#pytorchネットワークモジュール定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #入力チャネル=1(白黒仮定), 出力チャネル=32, カーネルサイズ=3, ストライド=1\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        #forward propagationでは確率p(=0.25)で入力が０となり、\n","        #それ以外は入力が1/(1-p)でスケールアップする。\n","        #back propagationでは勾配に対し確率pで０化され、それ以外は1/(1-p)でスケールアップする。\n","        self.dropout1 = nn.Dropout2d(0.10)\n","        self.dropout2 = nn.Dropout2d(0.25)\n","        #入力特徴数=9216、出力特徴数=128 全結合ネットワーク\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10) # 線形にする\n","\n","    def forward(self, x):\n","        #２回にわたり特徴面を拡大。\n","        #InputShape=28*28*1 OutputShape=26*26*32, Weights=(3*3*1)*32=288\n","        x = self.conv1(x)\n","        #torchが提供するランプ関数 負部分の値をゼロに平滑化する\n","        x = F.relu(x)\n","        #InputShape=26*26*32, OutputShape=24*24*64, Weights=(3*3*32)*64=18342\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        #カーネルサイズ=2 (入力2*2範囲内の最大値を1セルで置き換える)\n","        #InputShape=24*24*64, OutputShape=12*12*64(=9216)\n","        x = F.max_pool2d(x, 2)\n","        # x = self.dropout1(x)\n","        #InputShape=12*12*64, OutputShape=9216\n","        x = torch.flatten(x, 1)\n","        #Weight=9216*128=1179648\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        # x = self.dropout2(x)\n","        #Weghts=128*10=1280\n","        x = self.fc2(x)\n","        #-- Total Weights=1199558~=1.2M parameters\n","        #解説\n","        #softmax: e(x_i)/e(x).sum()\n","        #log_softmax: log(softmax(x))\n","        #dim=1 row-wise\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    #1エポックごとに呼び出される。\n","    #modelを学習用にセットアップする。\n","    model.train()\n","    #データをbatch-size単位に切り出し、(data, target)に再代入する\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        #data, targetをdevice（たとえばGPU）に展開する。\n","        data, target = data.to(device), target.to(device)\n","        #ミニバッチ単位の勾配計算に関し、勾配値の集積を毎回リセットする。\n","        optimizer.zero_grad()\n","        #出力を得る\n","        output = model(data)\n","        #損失を計算。 a:softmax(x), nll: -log(a)\n","        loss = F.nll_loss(output, target)\n","        #loss = F.cross_entropy(output, target)\n","        #勾配値を計算し、全パラメータを更新する。\n","        loss.backward()\n","        #指定された最適化関数の最適化方向へ勾配をシフト\n","        optimizer.step()\n","        #batch-size(=64)*log-interval(=50)ごとに損失率を記録。\n","        #loss.item() 損失平均\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","def test(model, device, test_loader):\n","    #学習時と異なるネットワーク構成（たとえばdropoutを含む）の場合。\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    #model.train()で学習済みのため、テスト時、勾配計算を省く。これによって推論時速度が上がる。\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            #test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n","            #出力中、もっとも高い確率を持つインデックスを知る。\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            #正解数を計算する。\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","def main():\n","    # Training settings\n","    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                        help='input batch size for training (default: 64)')\n","    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                        help='input batch size for testing (default: 1000)')\n","    # parser.add_argument('--epochs', type=int, default=2, metavar='N',\n","    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n","                        help='number of epochs to train (default: 14)')\n","    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n","                        help='learning rate (default: 1.0)') # 学習率\n","    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n","                        help='Learning rate step gamma (default: 0.7)')\n","    parser.add_argument('--no-cuda', action='store_true', default=False,\n","                        help='disables CUDA training')\n","    parser.add_argument('--dry-run', action='store_true', default=False,\n","                        help='quickly check a single pass')\n","    parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                        help='random seed (default: 1)')\n","    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n","                        help='how many batches to wait before logging training status')\n","    parser.add_argument('--save-model', action='store_true', default=False,\n","                        help='For Saving the current Model')\n","    ##本来、pythonインラインで実行を想定したプログラムをjupyterで動かす際の引数渡しに\n","    ##関する変更\n","    #args = parser.parse_args(), replaced with the following code for run in Jupyter\n","    args = parser.parse_args(args=[])\n","    #CUDA、つまりGPU使用の可否を確認。\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    # use_cuda = False\n","\n","    #全体で使用する乱数シードの設定。\n","    torch.manual_seed(args.seed)\n","\n","    #使用するデバイスの決定。cpuもしくはgpu\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    #cuda用学習時パラメータの用意\n","    #num_workers: 並列バッチ数\n","    kwargs = {'batch_size': args.batch_size}\n","    if use_cuda:\n","        kwargs.update({'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True},\n","                     )\n","    #入力データ(ndarray)をTensor値に変更\n","    #image = (image - mean) / std, here mena=0.1307, std=0.3081\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    #学習前データ\n","    dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","    #学習済みデータ\n","    dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)\n","    #ミニバッチを前提としたデータ取り込み\n","    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n","\n","    #netモデルをデバイスに展開\n","    model = Net().to(device)\n","    #最適化モデルの選択\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in tqdm.tqdm(range(1, args.epochs + 1)):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-ElFTOXmI9W","colab_type":"text"},"source":["↑ dropoutをなくした場合\n"]},{"cell_type":"code","metadata":{"id":"ER5YlgVQa0fG","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aN8asvvx1_Mf","colab_type":"text"},"source":["# 課題\n","元々のaccuracy: \n","\n","Test set: Average loss: 0.0359, Accuracy: 9870/10000 (99%)\n","\n","\n","① def trainのなかで....\n","\n","Test set: Average loss: 2.3118, Accuracy: 1066/10000 (11%)\n","\n","勾配値のリセットがなされないために勾配値が集積してしまい、使い物にならなくなってしまう\n","\n","② GPUを使わなかった場合、・・・\n","\n","\n","元々の時間：\n","\n","[00:29<00:00, 14.56s/it]\n","\n","GPUを使わなかった場合\n","[04:33<00:00, 136.53s/it]\n","\n","\n","\n","③ epochを14にした場合\n","\n","元々：　Test set: Average loss: 0.0359, Accuracy: 9870/10000 (99%)\n","\n","epoch 14: Test set: Average loss: 0.0265, Accuracy: 9918/10000 (99%)\n","\n","④ epoch 14 convolution2 をなくした場合\n","\n","Test set: Average loss: 0.0447, Accuracy: 9853/10000 (99%)\n","\n","⑤ epoch 14 linear2 をなくした場合\n","\n","Test set: Average loss: 0.9501, Accuracy: 6952/10000 (70%)\n","\n","⑥ dropout1 を　0.1に dropout2 を　0.25にした場合\n","\n","Test set: Average loss: 0.0265, Accuracy: 9917/10000 (99%)\n","\n","⑦ dropoutを二つともなくした場合\n","\n","Test set: Average loss: 0.0339, Accuracy: 9915/10000 (99%)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qFdj1Q0LSAY6","colab_type":"text"},"source":["<pre>\n","-- without cuda\n","  0%|          | 0/2 [00:00<?, ?it/s]Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293032\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.430521\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.293449\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.118110\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.274648\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.237137\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.327925\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.084112\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.101584\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.111421\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.285744\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.238111\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.148898\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.120191\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.245682\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.143803\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.320378\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.122081\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.176121\n"," 50%|█████     | 1/2 [01:55<01:55, 115.54s/it]\n","Test set: Average loss: 0.0506, Accuracy: 9840/10000 (98%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.137427\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.091146\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.239964\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.071102\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.078716\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.099546\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.194391\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.047520\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.038397\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.007070\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.088783\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.051199\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.063128\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.106921\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.181889\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.078312\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.053076\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.028962\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.056904\n","100%|██████████| 2/2 [03:58<00:00, 119.33s/it]\n","Test set: Average loss: 0.0393, Accuracy: 9877/10000 (99%)\n","\n","</pre>\n","\n","<pre>\n","-- with cuda\n"," 0%|          | 0/2 [00:00<?, ?it/s]Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333409\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.428493\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.325141\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.081500\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.269183\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.059994\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.229668\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.052204\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.161045\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.130063\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.133664\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.057657\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.107899\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.255379\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.021076\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.071690\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.147334\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.061756\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.099544\n"," 50%|█████     | 1/2 [00:12<00:12, 12.80s/it]\n","Test set: Average loss: 0.0571, Accuracy: 9817/10000 (98%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.035936\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.112590\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.063430\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.180120\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.018170\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.120949\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.052832\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.024330\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.016192\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.089144\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.087841\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.132195\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.075957\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.207489\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.076305\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.017077\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.163747\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.004621\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.232472\n","100%|██████████| 2/2 [00:25<00:00, 12.91s/it]\n","Test set: Average loss: 0.0356, Accuracy: 9867/10000 (99%)\n","</pre>"]},{"cell_type":"code","metadata":{"id":"p7cn91c8KEJW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}